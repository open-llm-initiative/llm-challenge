{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas transformers openai anthropic\n",
    "!pip install pytest\n",
    "!pip install transformers==4.42.3\n",
    "!pip install triton\n",
    "!pip install einops\n",
    "!pip install tiktoken\n",
    "!pip install pydantic==1.10.8\n",
    "!pip install flash-attn --no-build-isolation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "import openai\n",
    "import anthropic\n",
    "from openai import OpenAI\n",
    "\n",
    "import os\n",
    "#os.environ[\"HUGGING_FACE_HUB_TOKEN\"] = \"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\"\n",
    "#os.environ[\"ANTHROPIC_API_KEY\"]=\"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\"\n",
    "\n",
    "#os.environ[\"OPENAI_API_KEY\"]=\"sk-proj-1fcZQDT18ZuGzRsFoJRST3BlbkFJb3vsgq65YpOYfKzdZDwJ\"\n",
    "\n",
    "# Define model configurations\n",
    "\n",
    "#phi 3 only when instance has flashattention\n",
    "\n",
    "huggingface_models = {\n",
    "    #\"Phi-3-small-128k-instruct\":\"microsoft/Phi-3-small-128k-instruct\",\n",
    "    \"gemma-2-2B\": \"google/gemma-2-2b\",\n",
    "    \"Qwen2-0.5B\": \"Qwen/Qwen2-0.5B\",\n",
    "    \"Qwen2-0.5B-Instruct\": \"Qwen/Qwen2-0.5B-Instruct\",\n",
    "    \"Qwen2-1.5B\": \"Qwen/Qwen2-1.5B\",\n",
    "    \"Qwen2-7B-Instruct\": \"Qwen/Qwen2-7B-Instruct\",\n",
    "    #\"Qwen2-72B\": \"Qwen/Qwen2-72B\",\n",
    "    \"Meta-Llama-3.1-70B\": \"meta-llama/Meta-Llama-3.1-70B\"\n",
    "}\n",
    "\n",
    "anthropic_models = {\n",
    "    \"Claude 3 Haiku\": {\"api_key\": os.environ['ANTHROPIC_API_KEY'], \"model\": \"claude-3-haiku\"},\n",
    "    \"Claude 3.5 Sonnet\": {\"api_key\": os.environ['ANTHROPIC_API_KEY'], \"model\": \"claude-3.5-sonnet\"}\n",
    "}\n",
    "\n",
    "openai_models = {\n",
    "    \"OpenAI GPT 3.5-Turbo\": {\"api_key\": os.environ['OPENAI_API_KEY'], \"model\": \"gpt-3.5-turbo\"},\n",
    "    \"OpenAI GPT4-o\": {\"api_key\": os.environ['OPENAI_API_KEY'], \"model\": \"gpt-4\"}\n",
    "}\n",
    "\n",
    "# Initialize Hugging Face pipelines\n",
    "def initialize_huggingface_pipelines():\n",
    "    pipelines = {}\n",
    "    for model_name, model_id in huggingface_models.items():\n",
    "        try:\n",
    "            if model_name == \"Phi-3-small-128k-instruct\":\n",
    "                # Load with trust_remote_code=True for specific models\n",
    "                tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "                model = AutoModelForCausalLM.from_pretrained(model_id, trust_remote_code=True)\n",
    "            else:\n",
    "                tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "                model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "\n",
    "            pipelines[model_name] = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {model_name}: {str(e)}\")\n",
    "\n",
    "    return pipelines\n",
    "\n",
    "def call_huggingface_model(pipelines, model_name, question, context):\n",
    "    \"\"\"\n",
    "    Generate a response using a Hugging Face model.\n",
    "    \"\"\"\n",
    "    prompt = f\"{context}\\n\\nQuestion: {question}\"\n",
    "    response = pipelines[model_name](prompt, max_length=150,max_new_tokens=256, num_return_sequences=1, do_sample=True)\n",
    "    return response[0]['generated_text']\n",
    "\n",
    "def call_openai_model(model_name, question, context):\n",
    "    \"\"\"\n",
    "    Generate a response using an OpenAI model.\n",
    "    \"\"\"\n",
    "    # response = openai.ChatCompletion.create(\n",
    "    #     model=openai_models[model_name]['model'],\n",
    "    #     messages=[\n",
    "    #         {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    #         {\"role\": \"user\", \"content\": f\"{context}\\n\\nQuestion: {question}\"}\n",
    "    #     ]\n",
    "    # )\n",
    "    client = OpenAI(\n",
    "            api_key=openai_models[model_name]['api_key'],\n",
    "    )\n",
    "    response = client.chat.completions.create(\n",
    "                model=openai_models[model_name]['model'],\n",
    "                temperature = 0.1,\n",
    "                messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": f\"{context}\\n\\nQuestion: {question}\"}\n",
    "            ]\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def call_anthropic_model(model_name, question, context):\n",
    "    \"\"\"\n",
    "    Generate a response using an Anthropic Claude model.\n",
    "    \"\"\"\n",
    "    client = anthropic.Anthropic(api_key = anthropic_models[model_name]['api_key'])\n",
    "    prompt = f\"{context}\\n\\nQuestion: {question}\"\n",
    "    response = client.completions.create(\n",
    "        model=anthropic_models[model_name]['model'],\n",
    "        prompt=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": f\"{context}\\n\\nQuestion: {question}\"}\n",
    "            ],\n",
    "        max_tokens_to_sample=256,\n",
    "        stop_sequences=[anthropic.HUMAN_PROMPT]\n",
    "    )\n",
    "    return response.completion.strip()\n",
    "\n",
    "def generate_responses(df):\n",
    "    \"\"\"\n",
    "    Generate responses from all models for each question/context pair in the DataFrame.\n",
    "    \"\"\"\n",
    "    for model_name in anthropic_models.keys():\n",
    "        print(model_name)\n",
    "        df[f\"{model_name}_response\"] = df.apply(lambda row: call_anthropic_model(model_name, row['question'], row['context']), axis=1)\n",
    "    df.to_csv('questions_with_responses.csv', index=False)\n",
    "\n",
    "    for model_name in openai_models.keys():\n",
    "        print(model_name)\n",
    "        df[f\"{model_name}_response\"] = df.apply(lambda row: call_openai_model(model_name, row['question'], row['context']), axis=1)\n",
    "    df.to_csv('questions_with_responses.csv', index=False)\n",
    "    \n",
    "    # Initialize Hugging Face pipelines\n",
    "    huggingface_pipelines = initialize_huggingface_pipelines()\n",
    "\n",
    "    # Iterate through each question and context\n",
    "    for model_name in huggingface_models.keys():\n",
    "        print(model_name)\n",
    "        df[f\"{model_name}_response\"] = df.apply(lambda row: call_huggingface_model(huggingface_pipelines, model_name, row['question'], row['context']), axis=1)\n",
    "    df.to_csv('questions_with_responses.csv', index=False)\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claude 3 Haiku\n"
     ]
    },
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'type': 'error', 'error': {'type': 'invalid_request_error', 'message': 'Your credit balance is too low to access the Claude API. Please go to Plans & Billing to upgrade or purchase credits.'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m/home/sagemaker-user/rag_response.ipynb Cell 2\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell://0bxnd4i1cko1x1m.studio.us-east-1.sagemaker.aws/home/sagemaker-user/rag_response.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(\u001b[39m'\u001b[39m\u001b[39mllm_challenge.csv\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://0bxnd4i1cko1x1m.studio.us-east-1.sagemaker.aws/home/sagemaker-user/rag_response.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# Generate responses\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://0bxnd4i1cko1x1m.studio.us-east-1.sagemaker.aws/home/sagemaker-user/rag_response.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m df \u001b[39m=\u001b[39m generate_responses(df)\n\u001b[1;32m      <a href='vscode-notebook-cell://0bxnd4i1cko1x1m.studio.us-east-1.sagemaker.aws/home/sagemaker-user/rag_response.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m# Save the results to a new CSV\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://0bxnd4i1cko1x1m.studio.us-east-1.sagemaker.aws/home/sagemaker-user/rag_response.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m df\u001b[39m.\u001b[39mto_csv(\u001b[39m'\u001b[39m\u001b[39mquestions_with_responses.csv\u001b[39m\u001b[39m'\u001b[39m, index\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[1;32m/home/sagemaker-user/rag_response.ipynb Cell 2\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell://0bxnd4i1cko1x1m.studio.us-east-1.sagemaker.aws/home/sagemaker-user/rag_response.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=106'>107</a>\u001b[0m \u001b[39mfor\u001b[39;00m model_name \u001b[39min\u001b[39;00m anthropic_models\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m    <a href='vscode-notebook-cell://0bxnd4i1cko1x1m.studio.us-east-1.sagemaker.aws/home/sagemaker-user/rag_response.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=107'>108</a>\u001b[0m     \u001b[39mprint\u001b[39m(model_name)\n\u001b[0;32m--> <a href='vscode-notebook-cell://0bxnd4i1cko1x1m.studio.us-east-1.sagemaker.aws/home/sagemaker-user/rag_response.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=108'>109</a>\u001b[0m     df[\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mmodel_name\u001b[39m}\u001b[39;00m\u001b[39m_response\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39;49mapply(\u001b[39mlambda\u001b[39;49;00m row: call_anthropic_model(model_name, row[\u001b[39m'\u001b[39;49m\u001b[39mquestion\u001b[39;49m\u001b[39m'\u001b[39;49m], row[\u001b[39m'\u001b[39;49m\u001b[39mcontext\u001b[39;49m\u001b[39m'\u001b[39;49m]), axis\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[1;32m    <a href='vscode-notebook-cell://0bxnd4i1cko1x1m.studio.us-east-1.sagemaker.aws/home/sagemaker-user/rag_response.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=109'>110</a>\u001b[0m df\u001b[39m.\u001b[39mto_csv(\u001b[39m'\u001b[39m\u001b[39mquestions_with_responses.csv\u001b[39m\u001b[39m'\u001b[39m, index\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m    <a href='vscode-notebook-cell://0bxnd4i1cko1x1m.studio.us-east-1.sagemaker.aws/home/sagemaker-user/rag_response.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=111'>112</a>\u001b[0m \u001b[39mfor\u001b[39;00m model_name \u001b[39min\u001b[39;00m openai_models\u001b[39m.\u001b[39mkeys():\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/frame.py:10034\u001b[0m, in \u001b[0;36mDataFrame.apply\u001b[0;34m(self, func, axis, raw, result_type, args, by_row, **kwargs)\u001b[0m\n\u001b[1;32m  10022\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mapply\u001b[39;00m \u001b[39mimport\u001b[39;00m frame_apply\n\u001b[1;32m  10024\u001b[0m op \u001b[39m=\u001b[39m frame_apply(\n\u001b[1;32m  10025\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m  10026\u001b[0m     func\u001b[39m=\u001b[39mfunc,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m  10032\u001b[0m     kwargs\u001b[39m=\u001b[39mkwargs,\n\u001b[1;32m  10033\u001b[0m )\n\u001b[0;32m> 10034\u001b[0m \u001b[39mreturn\u001b[39;00m op\u001b[39m.\u001b[39;49mapply()\u001b[39m.\u001b[39m__finalize__(\u001b[39mself\u001b[39m, method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mapply\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/apply.py:837\u001b[0m, in \u001b[0;36mFrameApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    834\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw:\n\u001b[1;32m    835\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_raw()\n\u001b[0;32m--> 837\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_standard()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/apply.py:965\u001b[0m, in \u001b[0;36mFrameApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    964\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_standard\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 965\u001b[0m     results, res_index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_series_generator()\n\u001b[1;32m    967\u001b[0m     \u001b[39m# wrap results\u001b[39;00m\n\u001b[1;32m    968\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwrap_results(results, res_index)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/apply.py:981\u001b[0m, in \u001b[0;36mFrameApply.apply_series_generator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    978\u001b[0m \u001b[39mwith\u001b[39;00m option_context(\u001b[39m\"\u001b[39m\u001b[39mmode.chained_assignment\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    979\u001b[0m     \u001b[39mfor\u001b[39;00m i, v \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(series_gen):\n\u001b[1;32m    980\u001b[0m         \u001b[39m# ignore SettingWithCopy here in case the user mutates\u001b[39;00m\n\u001b[0;32m--> 981\u001b[0m         results[i] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunc(v, \u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkwargs)\n\u001b[1;32m    982\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(results[i], ABCSeries):\n\u001b[1;32m    983\u001b[0m             \u001b[39m# If we have a view on v, we need to make a copy because\u001b[39;00m\n\u001b[1;32m    984\u001b[0m             \u001b[39m#  series_generator will swap out the underlying data\u001b[39;00m\n\u001b[1;32m    985\u001b[0m             results[i] \u001b[39m=\u001b[39m results[i]\u001b[39m.\u001b[39mcopy(deep\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[1;32m/home/sagemaker-user/rag_response.ipynb Cell 2\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell://0bxnd4i1cko1x1m.studio.us-east-1.sagemaker.aws/home/sagemaker-user/rag_response.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=106'>107</a>\u001b[0m \u001b[39mfor\u001b[39;00m model_name \u001b[39min\u001b[39;00m anthropic_models\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m    <a href='vscode-notebook-cell://0bxnd4i1cko1x1m.studio.us-east-1.sagemaker.aws/home/sagemaker-user/rag_response.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=107'>108</a>\u001b[0m     \u001b[39mprint\u001b[39m(model_name)\n\u001b[0;32m--> <a href='vscode-notebook-cell://0bxnd4i1cko1x1m.studio.us-east-1.sagemaker.aws/home/sagemaker-user/rag_response.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=108'>109</a>\u001b[0m     df[\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mmodel_name\u001b[39m}\u001b[39;00m\u001b[39m_response\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m row: call_anthropic_model(model_name, row[\u001b[39m'\u001b[39;49m\u001b[39mquestion\u001b[39;49m\u001b[39m'\u001b[39;49m], row[\u001b[39m'\u001b[39;49m\u001b[39mcontext\u001b[39;49m\u001b[39m'\u001b[39;49m]), axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m    <a href='vscode-notebook-cell://0bxnd4i1cko1x1m.studio.us-east-1.sagemaker.aws/home/sagemaker-user/rag_response.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=109'>110</a>\u001b[0m df\u001b[39m.\u001b[39mto_csv(\u001b[39m'\u001b[39m\u001b[39mquestions_with_responses.csv\u001b[39m\u001b[39m'\u001b[39m, index\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m    <a href='vscode-notebook-cell://0bxnd4i1cko1x1m.studio.us-east-1.sagemaker.aws/home/sagemaker-user/rag_response.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=111'>112</a>\u001b[0m \u001b[39mfor\u001b[39;00m model_name \u001b[39min\u001b[39;00m openai_models\u001b[39m.\u001b[39mkeys():\n",
      "\u001b[1;32m/home/sagemaker-user/rag_response.ipynb Cell 2\u001b[0m line \u001b[0;36m9\n\u001b[1;32m     <a href='vscode-notebook-cell://0bxnd4i1cko1x1m.studio.us-east-1.sagemaker.aws/home/sagemaker-user/rag_response.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=89'>90</a>\u001b[0m client \u001b[39m=\u001b[39m anthropic\u001b[39m.\u001b[39mAnthropic(api_key \u001b[39m=\u001b[39m anthropic_models[model_name][\u001b[39m'\u001b[39m\u001b[39mapi_key\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m     <a href='vscode-notebook-cell://0bxnd4i1cko1x1m.studio.us-east-1.sagemaker.aws/home/sagemaker-user/rag_response.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=90'>91</a>\u001b[0m prompt \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mcontext\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mQuestion: \u001b[39m\u001b[39m{\u001b[39;00mquestion\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell://0bxnd4i1cko1x1m.studio.us-east-1.sagemaker.aws/home/sagemaker-user/rag_response.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=91'>92</a>\u001b[0m response \u001b[39m=\u001b[39m client\u001b[39m.\u001b[39;49mcompletions\u001b[39m.\u001b[39;49mcreate(\n\u001b[1;32m     <a href='vscode-notebook-cell://0bxnd4i1cko1x1m.studio.us-east-1.sagemaker.aws/home/sagemaker-user/rag_response.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=92'>93</a>\u001b[0m     model\u001b[39m=\u001b[39;49manthropic_models[model_name][\u001b[39m'\u001b[39;49m\u001b[39mmodel\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m     <a href='vscode-notebook-cell://0bxnd4i1cko1x1m.studio.us-east-1.sagemaker.aws/home/sagemaker-user/rag_response.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=93'>94</a>\u001b[0m     prompt\u001b[39m=\u001b[39;49m[\n\u001b[1;32m     <a href='vscode-notebook-cell://0bxnd4i1cko1x1m.studio.us-east-1.sagemaker.aws/home/sagemaker-user/rag_response.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=94'>95</a>\u001b[0m             {\u001b[39m\"\u001b[39;49m\u001b[39mrole\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39msystem\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mcontent\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39mYou are a helpful assistant.\u001b[39;49m\u001b[39m\"\u001b[39;49m},\n\u001b[1;32m     <a href='vscode-notebook-cell://0bxnd4i1cko1x1m.studio.us-east-1.sagemaker.aws/home/sagemaker-user/rag_response.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=95'>96</a>\u001b[0m             {\u001b[39m\"\u001b[39;49m\u001b[39mrole\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39muser\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mcontent\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m{\u001b[39;49;00mcontext\u001b[39m}\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39mQuestion: \u001b[39;49m\u001b[39m{\u001b[39;49;00mquestion\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m}\n\u001b[1;32m     <a href='vscode-notebook-cell://0bxnd4i1cko1x1m.studio.us-east-1.sagemaker.aws/home/sagemaker-user/rag_response.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=96'>97</a>\u001b[0m         ],\n\u001b[1;32m     <a href='vscode-notebook-cell://0bxnd4i1cko1x1m.studio.us-east-1.sagemaker.aws/home/sagemaker-user/rag_response.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=97'>98</a>\u001b[0m     max_tokens_to_sample\u001b[39m=\u001b[39;49m\u001b[39m256\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://0bxnd4i1cko1x1m.studio.us-east-1.sagemaker.aws/home/sagemaker-user/rag_response.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=98'>99</a>\u001b[0m     stop_sequences\u001b[39m=\u001b[39;49m[anthropic\u001b[39m.\u001b[39;49mHUMAN_PROMPT]\n\u001b[1;32m    <a href='vscode-notebook-cell://0bxnd4i1cko1x1m.studio.us-east-1.sagemaker.aws/home/sagemaker-user/rag_response.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=99'>100</a>\u001b[0m )\n\u001b[1;32m    <a href='vscode-notebook-cell://0bxnd4i1cko1x1m.studio.us-east-1.sagemaker.aws/home/sagemaker-user/rag_response.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=100'>101</a>\u001b[0m \u001b[39mreturn\u001b[39;00m response\u001b[39m.\u001b[39mcompletion\u001b[39m.\u001b[39mstrip()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/anthropic/_utils/_utils.py:277\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m             msg \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mMissing required argument: \u001b[39m\u001b[39m{\u001b[39;00mquote(missing[\u001b[39m0\u001b[39m])\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    276\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 277\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/anthropic/resources/completions.py:374\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, max_tokens_to_sample, model, prompt, metadata, stop_sequences, stream, temperature, top_k, top_p, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    372\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_given(timeout) \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_client\u001b[39m.\u001b[39mtimeout \u001b[39m==\u001b[39m DEFAULT_TIMEOUT:\n\u001b[1;32m    373\u001b[0m     timeout \u001b[39m=\u001b[39m \u001b[39m600\u001b[39m\n\u001b[0;32m--> 374\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_post(\n\u001b[1;32m    375\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39m/v1/complete\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    376\u001b[0m     body\u001b[39m=\u001b[39;49mmaybe_transform(\n\u001b[1;32m    377\u001b[0m         {\n\u001b[1;32m    378\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mmax_tokens_to_sample\u001b[39;49m\u001b[39m\"\u001b[39;49m: max_tokens_to_sample,\n\u001b[1;32m    379\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mmodel\u001b[39;49m\u001b[39m\"\u001b[39;49m: model,\n\u001b[1;32m    380\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mprompt\u001b[39;49m\u001b[39m\"\u001b[39;49m: prompt,\n\u001b[1;32m    381\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mmetadata\u001b[39;49m\u001b[39m\"\u001b[39;49m: metadata,\n\u001b[1;32m    382\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mstop_sequences\u001b[39;49m\u001b[39m\"\u001b[39;49m: stop_sequences,\n\u001b[1;32m    383\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mstream\u001b[39;49m\u001b[39m\"\u001b[39;49m: stream,\n\u001b[1;32m    384\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mtemperature\u001b[39;49m\u001b[39m\"\u001b[39;49m: temperature,\n\u001b[1;32m    385\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mtop_k\u001b[39;49m\u001b[39m\"\u001b[39;49m: top_k,\n\u001b[1;32m    386\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mtop_p\u001b[39;49m\u001b[39m\"\u001b[39;49m: top_p,\n\u001b[1;32m    387\u001b[0m         },\n\u001b[1;32m    388\u001b[0m         completion_create_params\u001b[39m.\u001b[39;49mCompletionCreateParams,\n\u001b[1;32m    389\u001b[0m     ),\n\u001b[1;32m    390\u001b[0m     options\u001b[39m=\u001b[39;49mmake_request_options(\n\u001b[1;32m    391\u001b[0m         extra_headers\u001b[39m=\u001b[39;49mextra_headers, extra_query\u001b[39m=\u001b[39;49mextra_query, extra_body\u001b[39m=\u001b[39;49mextra_body, timeout\u001b[39m=\u001b[39;49mtimeout\n\u001b[1;32m    392\u001b[0m     ),\n\u001b[1;32m    393\u001b[0m     cast_to\u001b[39m=\u001b[39;49mCompletion,\n\u001b[1;32m    394\u001b[0m     stream\u001b[39m=\u001b[39;49mstream \u001b[39mor\u001b[39;49;00m \u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    395\u001b[0m     stream_cls\u001b[39m=\u001b[39;49mStream[Completion],\n\u001b[1;32m    396\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/anthropic/_base_client.py:1266\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1252\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpost\u001b[39m(\n\u001b[1;32m   1253\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   1254\u001b[0m     path: \u001b[39mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1261\u001b[0m     stream_cls: \u001b[39mtype\u001b[39m[_StreamT] \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1262\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ResponseT \u001b[39m|\u001b[39m _StreamT:\n\u001b[1;32m   1263\u001b[0m     opts \u001b[39m=\u001b[39m FinalRequestOptions\u001b[39m.\u001b[39mconstruct(\n\u001b[1;32m   1264\u001b[0m         method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpost\u001b[39m\u001b[39m\"\u001b[39m, url\u001b[39m=\u001b[39mpath, json_data\u001b[39m=\u001b[39mbody, files\u001b[39m=\u001b[39mto_httpx_files(files), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions\n\u001b[1;32m   1265\u001b[0m     )\n\u001b[0;32m-> 1266\u001b[0m     \u001b[39mreturn\u001b[39;00m cast(ResponseT, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrequest(cast_to, opts, stream\u001b[39m=\u001b[39;49mstream, stream_cls\u001b[39m=\u001b[39;49mstream_cls))\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/anthropic/_base_client.py:942\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    933\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\n\u001b[1;32m    934\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    935\u001b[0m     cast_to: Type[ResponseT],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    940\u001b[0m     stream_cls: \u001b[39mtype\u001b[39m[_StreamT] \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    941\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ResponseT \u001b[39m|\u001b[39m _StreamT:\n\u001b[0;32m--> 942\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_request(\n\u001b[1;32m    943\u001b[0m         cast_to\u001b[39m=\u001b[39;49mcast_to,\n\u001b[1;32m    944\u001b[0m         options\u001b[39m=\u001b[39;49moptions,\n\u001b[1;32m    945\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    946\u001b[0m         stream_cls\u001b[39m=\u001b[39;49mstream_cls,\n\u001b[1;32m    947\u001b[0m         remaining_retries\u001b[39m=\u001b[39;49mremaining_retries,\n\u001b[1;32m    948\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/anthropic/_base_client.py:1046\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1043\u001b[0m         err\u001b[39m.\u001b[39mresponse\u001b[39m.\u001b[39mread()\n\u001b[1;32m   1045\u001b[0m     log\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mRe-raising status error\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 1046\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_status_error_from_response(err\u001b[39m.\u001b[39mresponse) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1048\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_process_response(\n\u001b[1;32m   1049\u001b[0m     cast_to\u001b[39m=\u001b[39mcast_to,\n\u001b[1;32m   1050\u001b[0m     options\u001b[39m=\u001b[39moptions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1053\u001b[0m     stream_cls\u001b[39m=\u001b[39mstream_cls,\n\u001b[1;32m   1054\u001b[0m )\n",
      "\u001b[0;31mBadRequestError\u001b[0m: Error code: 400 - {'type': 'error', 'error': {'type': 'invalid_request_error', 'message': 'Your credit balance is too low to access the Claude API. Please go to Plans & Billing to upgrade or purchase credits.'}}"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('llm_challenge.csv')\n",
    "\n",
    "# Generate responses\n",
    "df = generate_responses(df)\n",
    "# Save the results to a new CSV\n",
    "df.to_csv('questions_with_responses.csv', index=False)\n",
    "print(\"Responses have been generated and saved to 'questions_with_responses.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
